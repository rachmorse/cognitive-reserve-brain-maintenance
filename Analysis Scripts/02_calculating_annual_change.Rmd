---
title: "Annual Change"
output: html_document
date: "2024-04-26"
---
```{r setup, include=FALSE}
if (!require("ggplot2")) {
  install.packages("ggplot2")
  require("ggplot2")
}
if (!require("dplyr")) {
  install.packages("dplyr")
  require("dplyr")
}
if (!require("tidyr")) {
  install.packages("tidyr")
  require("tidyr")
}
```

Start by reading in the clean data in wide format 
```{r warning=FALSE}
betula_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Betula no outliers data wide.csv")

waha_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/WAHA no outliers data wide.csv")

cobra_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Cobra no outliers data wide.csv")

oslo_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Oslo no outliers data wide.csv")
```

Drop NAs - this assumes that to get a composite memory score they need to have all of the baseline data 
```{r}
drop_na_rows <- function(df, columns) {
  # Use complete.cases to drop rows with NA values in the specified columns
  df[complete.cases(df[, columns]), ]
}
waha_columns <- c("Verbal_memory_RAVLT_Total_baseline", "Verbal_Memory_RAVLT_delayed_baseline", "Visual_Memory_ROCF_Imm_baseline")
betula_columns <- c("CuedRecall_sptcrc_baseline", "CuedRecall_vtcrc_baseline", "FreeRecall_sptb_baseline", "FreeRecall_vtb_baseline")
oslo_columns <- c("CVLT_A_Total_baseline", "CVLT_5min_Free_baseline", "CVLT_30min_Free_baseline")
cobra_columns <- c("EM_verbal_baseline", "EM_numerical_baseline", "EM_figural_baseline")

oslo_df <- drop_na_rows(oslo_df, oslo_columns)
betula_df <- drop_na_rows(betula_df, betula_columns)
waha_df <- drop_na_rows(waha_df, waha_columns)
cobra_df <- drop_na_rows(cobra_df, cobra_columns)
```

Start by considering ICV for the HC measures

The formula looks like: 

Adjusted (HPC) volume = hc.1 (per participant) - b.1 (from the whole cohort) * (icv.1 (per participant) - mean_icv (from the whole cohort))
```{r}
# Start by making a merged df
merged_cohorts <- bind_rows(waha_df, cobra_df, betula_df, oslo_df)

# Step 1. Calculate Mean ICV by study / timepoint 
mean_icv_by_study_timepoint <- merged_cohorts %>%
  group_by(study) %>%
  summarise(across(starts_with("icv"), mean, na.rm = TRUE))

# Rename the columns
mean_icv_by_study_timepoint <- mean_icv_by_study_timepoint %>%
  rename_with(~ gsub("icv", "icv_mean", .), starts_with("icv"))

# Merge the mean ICV df in to the main df
merged_cohorts <- merged_cohorts %>%
  left_join(mean_icv_by_study_timepoint, by = "study")

# Step 2. Define a function to calculate the regression slope b for each study at each timepoint
calculate_slope <- function(df, hip_total_cols, icv_cols) {
  slopes <- data.frame() # Create an empty dataframe to store the slopes

  for (study in unique(df$study)) {
    for (i in seq_along(hip_total_cols)) {
      data <- df[df$study == study, c(hip_total_cols[i], icv_cols[i])]

      # Skip if there is no data for this study/timepoint
      if (nrow(data) == 0 || all(is.na(data[[hip_total_cols[i]]]), is.na(data[[icv_cols[i]]]))) {
        next
      }

      model <- lm(data[[hip_total_cols[i]]] ~ data[[icv_cols[i]]], data = data, na.action = na.exclude)
      slope <- coef(model)[2] # Calculate the slope

      slopes <- rbind(slopes, data.frame(study = study, timepoint = i, slope = slope)) # Add the slope to the dataframe
    }
  }

  return(slopes)
}

# Specify the columns to use for HIP_total (hc volume) and icv
hip_total_cols <- paste0("HIP_total.", 1:6)
icv_cols <- paste0("icv.", 1:6)

# Calculate the slope for each study
slopes <- calculate_slope(merged_cohorts, hip_total_cols, icv_cols)

# Convert the df to wide format to be able to merge it
slopes_wide <- slopes %>%
  pivot_wider(
    names_from = timepoint, values_from = c(-study, -timepoint),
    names_prefix = "slope.", names_sep = "_"
  )

# Merge the wide slope df into the main df
merged_cohorts <- merged_cohorts %>%
  left_join(slopes_wide, by = "study")

# Step 3. Iterate a formula over each timepoint to calculate adjusted hippocampal volumes for each participant
for (timepoint in 1:6) {
  raw_volume_col <- paste0("HIP_total.", timepoint)
  icv_col <- paste0("icv.", timepoint)
  icv_mean_col <- paste0("icv_mean.", timepoint)
  slope_col <- paste0("slope.", timepoint)
  adjusted_volume_col <- paste0("adj_HIP_total.", timepoint)

  merged_cohorts <- merged_cohorts %>%
    mutate(!!adjusted_volume_col := .data[[raw_volume_col]] - .data[[slope_col]] * (.data[[icv_col]] - .data[[icv_mean_col]]))
}

# Check the output data to see if it makes sense
correlation <- cor(merged_cohorts$HIP_total.4, merged_cohorts$adj_HIP_total.4, use = "complete.obs")
print(correlation)

ggplot(merged_cohorts, aes(x = HIP_total.4, y = adj_HIP_total.4)) +
  geom_point()
```
Now write a function to calculate the memory and hippocampus slopes for each cohort
```{r calculate slopes}
# Main processing function
process_data <- function(data, n_timepoints, mem_vars) {
  # Scale memory variables
  for (i in 1:n_timepoints) {
    for (mem_var in mem_vars) {
      base_col_name <- paste0(mem_var, "_baseline") # Baseline

      # Compute baseline value
      baseline_values <- data[[base_col_name]]

      # Current timepoint memory variable column
      col_name <- paste0(mem_var, ".", i)

      # Check if column exists
      if (col_name %in% names(data)) {
        # Calculate baseline mean and SD
        baseline_mean <- mean(data[[base_col_name]], na.rm = TRUE)
        baseline_sd <- sd(data[[base_col_name]], na.rm = TRUE)

        # Scale the variable using baseline mean and SD
        scaled_values <- (data[[col_name]] - baseline_mean) / baseline_sd
        data[[paste0("sc_", col_name)]] <- as.numeric(scaled_values) # Convert to numeric
      } else {
        cat("Column not found:", col_name, "\n")
      }
    }
  }

  # Calculate memory composites
  for (i in 1:n_timepoints) {
    mem_cols <- paste0("sc_", mem_vars, ".", i)
    data <- data %>%
      mutate(!!paste0("memory.", i) := rowMeans(
        select(., any_of(mem_cols)),
        na.rm = TRUE
      ))
  }

  # Calculate slopes
  for (i in 1:nrow(data)) {
    em_data <- data[i, paste0("memory.", 1:n_timepoints)]
    hc_data <- data[i, paste0("adj_HIP_total.", 1:n_timepoints)]
    age <- data[i, paste0("age.", 1:n_timepoints)]
    data[i, "em_obs"] <- sum(!is.na(em_data))
    data[i, "hc_obs"] <- sum(!is.na(hc_data))

    if (sum(!is.na(em_data)) > 1 & sum(!is.na(hc_data)) > 1) {
      data[i, "memory_slopes"] <- lm(em_data[!is.na(em_data)] ~ age[!is.na(em_data)])$coefficients[2]
      data[i, "hc_slopes"] <- lm(hc_data[!is.na(hc_data)] ~ age[!is.na(hc_data)])$coefficients[2]
      data[i, "mem_time"] <- max(age[!is.na(em_data)]) - min(age[!is.na(em_data)]) # Calculate follow-up time
      data[i, "hc_time"] <- max(age[!is.na(hc_data)]) - min(age[!is.na(hc_data)])
    }
  }
  data
}
```

Now calculate the slopes for memory change for participants for Oslo cohort:
```{r}
oslo_df <- merged_cohorts %>%
  filter(study %in% c("oslo_skyra", "oslo_avanto", "oslo_mix"))

# Define memory variables
memory_vars <- c("CVLT_5min_Free", "CVLT_A_Total", "CVLT_30min_Free")

# Specify the number of timepoints
n_timepoints <- 6

# Execute the process
oslo_df <- process_data(oslo_df, n_timepoints, memory_vars)
```

Now calculate the slope for participants for the Betula cohort
```{r}
betula_df <- merged_cohorts %>%
  filter(study == "betula")

# Rename the Betula variables for consistency (because they were renamed for the creation of the baseline variables)
for (i in 1:2) {
  old_name_sptcrc <- paste0("SPT_VTCategoryCuedRecall..sptcrc.", i)
  new_name_sptcrc <- paste0("CuedRecall_sptcrc.", i)

  old_name_vtcrc <- paste0("SPT_VTCategoryCuedRecall..vtcrc.", i)
  new_name_vtcrc <- paste0("CuedRecall_vtcrc.", i)

  old_name_sptb <- paste0("SPT_VTFreeRecall..sptb.", i)
  new_name_sptb <- paste0("FreeRecall_sptb.", i)

  old_name_vtb <- paste0("SPT_VTFreeRecall..vtb.", i)
  new_name_vtb <- paste0("FreeRecall_vtb.", i)

  betula_df <- betula_df %>%
    rename(
      !!new_name_sptcrc := all_of(old_name_sptcrc),
      !!new_name_vtcrc := all_of(old_name_vtcrc),
      !!new_name_sptb := all_of(old_name_sptb),
      !!new_name_vtb := all_of(old_name_vtb)
    )
}

# Define memory variables
memory_vars <- c("CuedRecall_sptcrc", "CuedRecall_vtcrc", "FreeRecall_sptb", "FreeRecall_vtb")

# Specify the number of timepoints
n_timepoints <- 2

# Execute the process
betula_df <- process_data(betula_df, n_timepoints, memory_vars)
```

And do the same for the WAHA cohort 
```{r}
waha_df <- merged_cohorts %>%
  filter(study == "waha")

# Define memory variables
memory_vars <- c("Verbal_memory_RAVLT_Total", "Verbal_Memory_RAVLT_delayed", "Visual_Memory_ROCF_Imm")

# Specify the number of timepoints
n_timepoints <- 3

# Execute the process
waha_df <- process_data(waha_df, n_timepoints, memory_vars)
```

Now create the Cobra one
```{r}
cobra_df <- merged_cohorts %>%
  filter(study == "cobra")

# Define memory variables
memory_vars <- c("EM_numerical", "EM_figural", "EM_verbal")

# Specify the number of timepoints
n_timepoints <- 2

# Execute the process
cobra_df <- process_data(cobra_df, n_timepoints, memory_vars)
```

Then merge the cohorts back together into one dataframe
```{r}
merged_cohorts <- bind_rows(waha_df, cobra_df, betula_df, oslo_df)
```

Then create average functional connectivity values for the DMN 
```{r}
# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
  # Calculate the average of all dDMN and vDMN columns and save it as DMN
  merged_cohorts[paste0("dmn.", i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(dDMN|vDMN)_.*\\.(dDMN|vDMN)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE)
}
```

Then create average functional connectivity values for the Executive Control Network 
```{r}
# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
  # Calculate the average of all RECN and LECN columns and save it as EC
  merged_cohorts[paste0("ec.", i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(RECN|LECN)_.*\\.(RECN|LECN)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE)
}
```

Then create average functional connectivity values for the Anterior Salience Network 
```{r}
# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
  # Calculate the average of all RECN and LECN columns and save it as EC
  merged_cohorts[paste0("sn.", i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(AS)_.*\\.(AS)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE)
}
```

Now calculate the slope for FC annual change
```{r}
compute_fc_stats <- function(data) {
  # Calculate slopes
  for (i in 1:nrow(data)) {
    dmn_data <- data[i, c(paste0("dmn.", 1:6))]
    age <- data[i, c(paste0("age.", 1:6))]
    data[i, "fc_obs"] <- sum(!is.na(dmn_data))

    if (sum(!is.na(dmn_data)) > 1) {
      data[i, "fc_time"] <- max(age[!is.na(dmn_data)]) - min(age[!is.na(dmn_data)])
      }
    }
  data <- data %>% ungroup()
  return(data)
}

merged_cohorts <- compute_fc_stats(merged_cohorts)
```

Then clean a main dataframe for fc
```{r}
# Create a new dataframe
clean_data <- select(
  merged_cohorts,
  id, study, sex, edu, mmse, cohort,
  memory_slopes, hc_slopes,
  em_obs, hc_obs, fc_obs,
  mem_time, hc_time, fc_time,
  paste0("age.", 1:6),
  paste0("memory.", 1:6),
  paste0("adj_HIP_total.", 1:6),
  paste0("HIP_total.", 1:6),
  paste0("icv.", 1:6),
  paste0("ec.", 1:6),
  paste0("sn.", 1:6),
  paste0("dmn.", 1:6)
)

# Drop people who are missing all necessary data
clean_data <- clean_data %>%
  filter(!(is.na(memory_slopes) & is.na(hc_slopes))) %>% 
  filter(em_obs >= 2, hc_obs >= 2) # Assumes that they need to have at least two EM and two HC observations for analyses

# Drop people who have a FU time of less than 1.5 years
clean_data <- clean_data %>%
  filter(mem_time > 1.5)
```

Find the baseline and followup values because not everyone has tp1 data and follow-up could be tp2 - tp6

Note that this needs to be used carefully and any code comparing BL and FU values must make sure that the number of observations for the variables are the same because, for example, if someone has only two memory values but three HC values, this code will pull the tp1 and tp2 memory values but the tp1 and tp3 HC values so in this case the HC and memory FU could not be compared. 
```{r}
# Function to create baseline and followup columns dynamically
create_bl_fu_columns <- function(data) {
  # Variables/Patterns to process
  variables <- c("adj_HIP_total", "memory", "age")

  # Create new columns
  for (var in variables) {
    data[[paste0(var, "_baseline")]] <- NA
    data[[paste0(var, "_followup")]] <- NA
  }

  # Helper function to assign baseline and followup values
  assign_columns <- function(row, var, original_data, data) {
    pattern <- paste0("^", var, "\\.\\d+$") # \\d selects any digit <10 at the end of str $
    indices <- grep(pattern, names(original_data))
    values <- original_data[row, indices, drop = FALSE]

    non_na_indices <- which(!is.na(values))

    if (length(non_na_indices) >= 1) {
      baseline_col <- paste0(var, "_baseline")
      baseline_index <- indices[non_na_indices[1]]
      data[[baseline_col]][row] <- original_data[row, baseline_index]
    }
    if (length(non_na_indices) > 1) {
      followup_col <- paste0(var, "_followup")
      followup_index <- indices[non_na_indices[length(non_na_indices)]]
      data[[followup_col]][row] <- original_data[row, followup_index]
    }

    return(data)
  }

  # Iterate over each row and variables
  for (i in 1:nrow(data)) {
    for (var in variables) {
      data <- assign_columns(i, var, data, data)
    }
  }

  return(data)
}

# Apply the function
clean_data <- clean_data %>%
  create_bl_fu_columns()

# Check that it is working correctly
age_check <- clean_data %>%
  select(id, matches("^age\\.\\d+$"), age_baseline, age_followup)
```

```{r}
# FU time
# Create a new variable to represent average follow-up time
clean_data <- clean_data %>%
  mutate(FU_time = rowMeans(select(., mem_time, hc_time, fc_time), na.rm = TRUE))

# FU observations
# Create a new variable to represent average number of follow-ups
clean_data <- clean_data %>%
  mutate(FU_obs = rowMeans(select(., em_obs, hc_obs, fc_obs), na.rm = TRUE))
```

Now write the data frames as CSVs
```{r}
write.csv(clean_data, file = "/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Merged cohorts analysed data wide.csv")
```
