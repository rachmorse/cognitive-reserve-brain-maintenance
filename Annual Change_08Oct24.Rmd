---
title: "Annual Change"
output: html_document
date: "2024-04-26"
---
```{r setup, include=FALSE}
if (!require("ggplot2")) {install.packages("ggplot2"); require("ggplot2")}
if (!require("dplyr")) {install.packages("dplyr"); require("dplyr")}
if (!require("stringr")) {install.packages("stringr"); require("stringr")}
if (!require("tidyr")) {install.packages("tidyr"); require("tidyr")}
if (!require("kableExtra")) {install.packages("kableExtra"); require("kableExtra")}
if (!require("knitr")) {install.packages("knitr"); require("knitr")}
if (!require("broom")) {install.packages("broom"); require("broom")}


library(readr)
library(readxl)
```

Start by reading in the clean data in wide format 
```{r warning=FALSE}

betula_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Betula no outliers data wide.csv")

waha_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/WAHA no outliers data wide.csv")

cobra_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Cobra no outliers data wide.csv")

oslo_df <- read.csv("/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Oslo no outliers data wide.csv")

```

Drop NAs - this assumes that to get a memory score they need to have all of the baseline data 
```{r}

drop_na_rows <- function(df, columns) {
  # Use complete.cases to drop rows with NA values in the specified columns
  df[complete.cases(df[, columns]), ]
}
waha_columns <- c("Verbal_memory_RAVLT_total_baseline", "Verbal_memory_RAVLT_delayed_baseline", "Visual_Memory_ROCF_Imm_baseline")
betula_columns <- c("CuedRecall_sptcrc_baseline", "CuedRecall_vtcrc_baseline", "FreeRecall_sptb_baseline", "FreeRecall_vtb_baseline")
oslo_columns <- c("CVLT_A_Total_baseline", "CVLT_5min_Free_baseline", "CVLT_30min_Free_baseline")
cobra_columns <- c("EM_verbal.1", "EM_numerical.1", "EM_figural.1")

oslo_df <- drop_na_rows(oslo_df, oslo_columns)
betula_df <- drop_na_rows(betula_df, betula_columns)
waha_df <- drop_na_rows(waha_df, waha_columns)
cobra_df <- drop_na_rows(cobra_df, cobra_columns)

```

Start by considering ICV for the HC measures

The formula looks like: 

Adjusted (HPC) volume = hc.1 (per participant) - b.1 (from the whole cohort) * (icv.1 (per participant) - mean_icv (from the whole cohort))
Adjusted (HPC) volume = hc.2 - b.2 * (icv.1 (because it does not change from tp1) - mean_icv)

```{r}

# Start by making a merged df
merged_cohorts <- bind_rows(waha_df, cobra_df, betula_df, oslo_df)

#### Calculate Mean ICV by study and timepoint (e.g. Cobra gets a mean ICV for each timepoint) ####
mean_icv_by_study_timepoint <- merged_cohorts %>%
  group_by(study) %>%
  summarise(across(starts_with("icv"), mean, na.rm = TRUE))

# Rename the columns
mean_icv_by_study_timepoint <- mean_icv_by_study_timepoint %>%
  rename_with(~ gsub("icv", "icv_mean", .), starts_with("icv"))

# Merge the mean ICV df in to the main df 
merged_cohorts <- merged_cohorts %>%
  left_join(mean_icv_by_study_timepoint, by = "study")

#### Define a function to calculate the regression slope b for each study at each timepoint ####
calculate_slope <- function(df, hip_total_cols, icv_cols) {
  slopes <- data.frame()  # Create an empty dataframe to store the slopes
  
  for (study in unique(df$study)) {
    for (i in seq_along(hip_total_cols)) {
      data <- df[df$study == study, c(hip_total_cols[i], icv_cols[i])]
      
      # Skip if there is no data for this study/timepoint
      if (nrow(data) == 0 || all(is.na(data[[hip_total_cols[i]]]), is.na(data[[icv_cols[i]]]))) {
        next
      }
      
      model <- lm(data[[hip_total_cols[i]]] ~ data[[icv_cols[i]]], data = data, na.action = na.exclude)
      slope <- coef(model)[2]  # Calculate the slope
      
      slopes <- rbind(slopes, data.frame(study = study, timepoint = i, slope = slope))  # Add the slope to the dataframe
    }
  }
  
  return(slopes)
}

# Specify the columns to use for HIP_total (hc volume) and icv
hip_total_cols <- paste0("HIP_total.", 1:6)
icv_cols <- paste0("icv.", 1:6)

# Calculate the slope for each study
slopes <- calculate_slope(merged_cohorts, hip_total_cols, icv_cols)

# Convert the df to wide format to be able to merge it
slopes_wide <- slopes %>%
  pivot_wider(names_from = timepoint, values_from = c(-study, -timepoint), 
              names_prefix = "slope.", names_sep = "_")

# Merge the wide slope df into the main df
merged_cohorts <- merged_cohorts %>%
  left_join(slopes_wide, by = "study")

#### Iterate a formula over each timepoint to calculate adjusted hippocampal volumes for each participant ####

for (timepoint in 1:6) {
  raw_volume_col <- paste0("HIP_total.", timepoint)
  icv_col <- paste0("icv.", timepoint)
  icv_mean_col <- paste0("icv_mean.", timepoint)
  slope_col <- paste0("slope.", timepoint)
  adjusted_volume_col <- paste0("adj_HIP_total.", timepoint)
  
  merged_cohorts <- merged_cohorts %>%
    mutate(!!adjusted_volume_col := .data[[raw_volume_col]] - .data[[slope_col]] * (.data[[icv_col]] - .data[[icv_mean_col]]))
}

#### Check the output data to see if it makes sense ####
correlation <- cor(merged_cohorts$HIP_total.4, merged_cohorts$adj_HIP_total.4, use = "complete.obs")
print(correlation)

ggplot(merged_cohorts, aes(x = HIP_total.4, y = adj_HIP_total.4)) + 
  geom_point()

```


Now calculate the of slopes for memory change for participants for Oslo cohort:
```{r}

oslo_df <- merged_cohorts %>% 
  filter(study %in% c("oslo_skyra", "oslo_avanto", "oslo_mix"))

compute_slope_oslo <- function(data){ # scale first
  data[,c(paste0("sc_CVLT_A_Total.", 1:6))] <- 
    apply(data[,c(paste0("CVLT_A_Total.", 1:6))], 2, scale, 
          center = mean(data$CVLT_A_Total_baseline),
          scale = sd(data$CVLT_A_Total_baseline))
  
  data[,c(paste0("sc_CVLT_5min_Free.", 1:6))] <- 
    apply(data[,c(paste0("CVLT_5min_Free.", 1:6))], 2, scale, 
          center = mean(data$CVLT_5min_Free_baseline),
          scale = sd(data$CVLT_5min_Free_baseline))
  
  data[,c(paste0("sc_CVLT_30min_Free.", 1:6))] <- 
    apply(data[,c(paste0("CVLT_30min_Free.", 1:6))], 2, scale, 
          center = mean(data$CVLT_30min_Free_baseline),
          scale = sd(data$CVLT_30min_Free_baseline))

  # Calculate memory composite
  for(i in 1:6) {
    data <- data %>%
      mutate(!!paste0("memory.", i) := rowMeans(
        dplyr::select(
          .,
          paste0("sc_CVLT_A_Total.", i),
          paste0("sc_CVLT_5min_Free.", i),
          paste0("sc_CVLT_30min_Free.", i)
        ),
        na.rm = TRUE
      ))
  }

    # Calculate slopes
    for (i in 1:nrow(data)){
      em_data = data[i,c(paste0("memory.", 1:6))]
      hc_data = data[i,c(paste0("adj_HIP_total.", 1:6))]
      age = data[i, c(paste0("age.", 1:6))]
      data[i, "em_obs"] <- sum(!is.na(em_data))
      data[i, "hc_obs"] <- sum(!is.na(hc_data))
      if (sum(!is.na(em_data))>1 & sum(!is.na(hc_data))>1){
      data[i, "mem_time"] <- max(age[!is.na(em_data)]) - min(age[!is.na(em_data)]) # to calculate follow-up time
      data[i, "hc_time"] <- max(age[!is.na(hc_data)]) - min(age[!is.na(hc_data)])
      data[i, "memory_slopes"] <- lm(em_data[!is.na(em_data)]~ age[!is.na(em_data)])$coeff[2]
      data[i, "hc_slopes"] <- lm(hc_data[ !is.na(hc_data)]~ age[ !is.na(hc_data)])$coeff[2]
      }
    }
    data %>%
    ungroup()
  
  return(data)
}

oslo_df <- compute_slope_oslo(oslo_df)

```

Now calculate the slope for participants for the Betula cohort
```{r}
betula_df <- merged_cohorts %>% 
  filter(study == "betula")

compute_slope_betula <- function(data){
  data[, c(paste0("sc_CuedRecall_sptcrc.", 1:2))] <- 
    apply(data[,c(paste0("SPT_VTCategoryCuedRecall..sptcrc.", 1:2))], 2, scale, 
          center = mean(data$"CuedRecall_sptcrc_baseline", na.rm=T),
          scale = sd(data$"CuedRecall_sptcrc_baseline", na.rm=T))
  
  data[, c(paste0("sc_CuedRecall_vtcrc.", 1:2))] <- 
    apply(data[,c(paste0("SPT_VTCategoryCuedRecall..vtcrc.", 1:2))], 2, scale, 
          center = mean(data$"CuedRecall_vtcrc_baseline", na.rm=T),
          scale = sd(data$"CuedRecall_vtcrc_baseline", na.rm=T))
  
  data[, c(paste0("sc_FreeRecall_sptb.", 1:2))] <- 
  apply(data[,c(paste0("SPT_VTFreeRecall..sptb.", 1:2))], 2, scale, 
        center = mean(data$"FreeRecall_sptb_baseline", na.rm=T),
        scale = sd(data$"FreeRecall_sptb_baseline", na.rm=T))
  
  data[, c(paste0("sc_FreeRecall_vtb.", 1:2))] <- 
  apply(data[,c(paste0("SPT_VTFreeRecall..vtb.", 1:2))], 2, scale, 
        center = mean(data$"FreeRecall_vtb_baseline", na.rm=T),
        scale = sd(data$"FreeRecall_vtb_baseline", na.rm=T))
  
    # Calculate memory composite
  for(i in 1:2) {
    data <- data %>%
      mutate(!!paste0("memory.", i) := rowMeans(
        dplyr::select(
          .,
          paste0("sc_CuedRecall_sptcrc.", i),
          paste0("sc_CuedRecall_vtcrc.", i),
          paste0("sc_FreeRecall_sptb.", i),
          paste0("sc_FreeRecall_vtb.", i)
        ),
        na.rm = TRUE
      ))
  }
  
        # Calculate slopes
    for (i in 1:nrow(data)){
      em_data = data[i,c(paste0("memory.", 1:2))]
      hc_data = data[i,c(paste0("adj_HIP_total.", 1:2))]
      age = data[i, c(paste0("age.", 1:2 ))]
      data[i, "em_obs"] <- sum(!is.na(em_data))
      data[i, "hc_obs"] <- sum(!is.na(hc_data))
      if (sum(!is.na(em_data))>1 & sum(!is.na(hc_data))>1){
      data[i, "mem_time"] <- max(age[!is.na(em_data)]) - min(age[!is.na(em_data)]) # to calculate follow-up time
      data[i, "hc_time"] <- max(age[!is.na(hc_data)]) - min(age[!is.na(hc_data)])
      data[i, "memory_slopes"] <- lm(em_data[!is.na(em_data)]~ age[!is.na(em_data)])$coeff[2]
      data[i, "hc_slopes"] <- lm(hc_data[ !is.na(hc_data)]~ age[ !is.na(hc_data)])$coeff[2]
      }
    }
    data %>%
    ungroup()

  return(data)
}

betula_df <- compute_slope_betula(betula_df)

```

And do the same for the Waha cohort 
```{r}
waha_df <- merged_cohorts %>% 
  filter(study == "waha")

compute_slope_waha <- function(data){ # scale first
    data[, c(paste0("sc_Verbal_memory_RAVLT_total.", 1:3))] <- 
      apply(data[,c(paste0("Verbal_memory_RAVLT_Total.", 1:3))], 2, scale, 
            center = mean(data$Verbal_memory_RAVLT_total_baseline, na.rm=T),
            scale = sd(data$Verbal_memory_RAVLT_total_baseline, na.rm=T))
    
    data[, c(paste0("sc_Verbal_memory_RAVLT_delayed.", 1:3))] <- 
      apply(data[,c(paste0("Verbal_Memory_RAVLT_delayed.", 1:3))], 2, scale, 
            center = mean(data$Verbal_memory_RAVLT_delayed_baseline, na.rm=T),
            scale = sd(data$Verbal_memory_RAVLT_delayed_baseline, na.rm=T))
        
    data[, c(paste0("sc_Visual_Memory_ROCF_Imm.", 1:3))] <- 
      apply(data[,c(paste0("Visual_Memory_ROCF_Imm.", 1:3))], 2, scale, 
            center = mean(data$Visual_Memory_ROCF_Imm_baseline, na.rm=T),
            scale = sd(data$Visual_Memory_ROCF_Imm_baseline, na.rm=T))
    
      # Calculate memory composite
    for(i in 1:3) {
      data <- data %>%
        mutate(!!paste0("memory.", i) := rowMeans(
          dplyr::select(
            .,
            paste0("sc_Verbal_memory_RAVLT_total.", i),
            paste0("sc_Verbal_memory_RAVLT_delayed.", i),
            paste0("sc_Visual_Memory_ROCF_Imm.", i)
          ),
          na.rm = TRUE
        ))
    }
  
        # Calculate slopes
    for (i in 1:nrow(data)){
      em_data = data[i,c(paste0("memory.", 1:3))]
      hc_data = data[i,c(paste0("adj_HIP_total.", 1:3))]
      age = data[i, c(paste0("age.", 1:3))]
      data[i, "em_obs"] <- sum(!is.na(em_data))
      data[i, "hc_obs"] <- sum(!is.na(hc_data))
      if (sum(!is.na(em_data))>1 & sum(!is.na(hc_data))>1){
      data[i, "mem_time"] <- max(age[!is.na(em_data)]) - min(age[!is.na(em_data)]) # to calculate follow-up time
      data[i, "hc_time"] <- max(age[!is.na(hc_data)]) - min(age[!is.na(hc_data)])
      data[i, "memory_slopes"] <- lm(em_data[!is.na(em_data)]~ age[!is.na(em_data)])$coeff[2]
      data[i, "hc_slopes"] <- lm(hc_data[ !is.na(hc_data)]~ age[ !is.na(hc_data)])$coeff[2]
      }
    }
    data %>%
    ungroup()

  return(data)
}

waha_df <- compute_slope_waha(waha_df)


```

Now create the Cobra one
```{r}
cobra_df <- merged_cohorts %>% 
  filter(study == "cobra")

compute_slope_cobra <- function(data){ # scale first
    data[, c(paste0("sc_EM_numerical.", 1:2))] <- 
      apply(data[,c(paste0("EM_numerical.", 1:2))], 2, scale, 
            center = mean(data$EM_numerical.1, na.rm=T),
            scale = sd(data$EM_numerical.1, na.rm=T))
    
    data[, c(paste0("sc_EM_figural.", 1:2))] <- 
      apply(data[,c(paste0("EM_figural.", 1:2))], 2, scale, 
            center = mean(data$EM_figural.1, na.rm=T),
            scale = sd(data$EM_figural.1, na.rm=T))
    
    data[, c(paste0("sc_EM_verbal.", 1:2))] <- 
      apply(data[,c(paste0("EM_verbal.", 1:2))], 2, scale, 
            center = mean(data$EM_verbal.1, na.rm=T),
            scale = sd(data$EM_verbal.1, na.rm=T))
        
        # Calculate memory composite
    for(i in 1:2) {
      data <- data %>%
        mutate(!!paste0("memory.", i) := rowMeans(
          dplyr::select(
            .,
            paste0("sc_EM_numerical.", i),
            paste0("sc_EM_figural.", i),
            paste0("sc_EM_verbal.", i)
          ),
          na.rm = TRUE
        ))
    }
    
        # Calculate slopes
    for (i in 1:nrow(data)){
      em_data = data[i,c(paste0("memory.", 1:2))]
      hc_data = data[i,c(paste0("adj_HIP_total.", 1:2))]
      age = data[i, c(paste0("age.", 1:2))]
      data[i, "em_obs"] <- sum(!is.na(em_data))
      data[i, "hc_obs"] <- sum(!is.na(hc_data))
      if (sum(!is.na(em_data))>1 & sum(!is.na(hc_data))>1){
      data[i, "mem_time"] <- max(age[!is.na(em_data)]) - min(age[!is.na(em_data)]) # to calculate follow-up time
      data[i, "hc_time"] <- max(age[!is.na(hc_data)]) - min(age[!is.na(hc_data)])
      data[i, "memory_slopes"] <- lm(em_data[!is.na(em_data)]~ age[!is.na(em_data)])$coeff[2]
      data[i, "hc_slopes"] <- lm(hc_data[ !is.na(hc_data)]~ age[ !is.na(hc_data)])$coeff[2]
      }
    }
    data %>%
    ungroup()

  return(data)
}

cobra_df <- compute_slope_cobra(cobra_df)

```

Then merge the cohorts back together into one dataframe
```{r}

merged_cohorts <- bind_rows(waha_df, cobra_df, betula_df, oslo_df)

```

Make a graph showing what one linear regression looks like (for the purpose of demonstrating how the annual change formula works)
```{r}
id <- "oslo_1100424" # pick a random subject

# Filter data for one subject
subject_data <- merged_cohorts[merged_cohorts$id == id, ]

# Concatenate the memory and age columns to create numeric vectors
hc_data <- unlist(subject_data[paste0("adj_HIP_total.", 1:6)])
age <- unlist(subject_data[paste0("age.", 1:6)])

# Remove any NAs
hc_data <- na.omit(hc_data)
age <- na.omit(age)

# Calculate time vector by setting the minimum age to 0 (eg BL)
time <- age - min(age)

# Combine the vectors into a df
plot_data <- data.frame(Time = time, HC = hc_data)
plot_data <- na.omit(plot_data)

# Fit the linear model for the subject
fit <- lm(HC ~ Time, data = plot_data)

# Plot 
ggplot(plot_data, aes(x = Time, y = HC)) +
  geom_point(color = "red") +
  geom_smooth(method = "lm", color = "blue", se = FALSE) +
  labs(
    title = paste("Hippocampus Annual Change (Slope) for Example Subject"),
    x = "Time Since Baseline",
    y = "Adjusted Hippocampal Volume"
  ) +
  annotate("text", x = max(time), y = min(hc_data), 
           label = paste("Slope:", round(coef(fit)[2], 2)), 
           color = "black", hjust = 1.1, vjust = 2.5)
```

Then create average functional connectivity values for the DMN 
```{r}

# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
# Calculate the average of all dDMN and vDMN columns and save it as DMN
  merged_cohorts[paste0('dmn.', i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(dDMN|vDMN)_.*\\.(dDMN|vDMN)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE) 
}

# Check that the output is ok
summary(merged_cohorts$dmn.6)

```

Then create average functional connectivity values for the Executive Control Network 
```{r}

# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
# Calculate the average of all RECN and LECN columns and save it as EC
  merged_cohorts[paste0('ec.', i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(RECN|LECN)_.*\\.(RECN|LECN)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE)}

# Check that the output is ok
summary(merged_cohorts$ec.5)

```

Then create average functional connectivity values for the Anterior Salience Network 
```{r}

var_list <- merged_cohorts[, grepl(paste0("^(AS)_.*\\.(AS)_.*\\.", i, "$"), names(merged_cohorts))]

# Loop the code making the averages over the 6 timepoints for each participant
for (i in 1:6) {
# Calculate the average of all RECN and LECN columns and save it as EC
  merged_cohorts[paste0('sn.', i)] <- rowMeans(merged_cohorts[, grepl(paste0("^(AS)_.*\\.(AS)_.*\\.", i, "$"), names(merged_cohorts))], na.rm = TRUE)}

# Check that the output is ok
summary(merged_cohorts$sn.5)

```

Now calculate the slope for FC annual change

```{r}

compute_fc_slope <- function(data) {
  # Calculate slopes
  for (i in 1:nrow(data)) {
    dmn_data <- data[i, c(paste0("dmn.", 1:6))]
    ec_data <- data[i, c(paste0("ec.", 1:6))]
    sn_data <- data[i, c(paste0("sn.", 1:6))]
    age <- data[i, c(paste0("age.", 1:6))]
    data[i, "fc_obs"] <- sum(!is.na(dmn_data))
    
    if (sum(!is.na(dmn_data)) > 1) {
      data[i, "fc_time"] <- max(age[!is.na(dmn_data)]) - min(age[!is.na(dmn_data)])
      
      if (sum(!is.na(dmn_data)) > 1) {
        data[i, "dmn_slopes"] <- lm(dmn_data[!is.na(dmn_data)] ~ age[!is.na(dmn_data)])$coeff[2]
      }
      if (sum(!is.na(ec_data)) > 1) {
        data[i, "ec_slopes"] <- lm(ec_data[!is.na(ec_data)] ~ age[!is.na(ec_data)])$coeff[2]
      }
      if (sum(!is.na(sn_data)) > 1) {
        data[i, "sn_slopes"] <- lm(sn_data[!is.na(sn_data)] ~ age[!is.na(sn_data)])$coeff[2]
      }
    }
  }
  data <- data %>% ungroup()
  return(data)
}

merged_cohorts <- compute_fc_slope(merged_cohorts)

```

Then clean a main dataframe for fc
```{r}

# Create a new dataframe

clean_data <- select(merged_cohorts,
                     id, study, sex,edu, mmse, cohort,
                     dmn_slopes, ec_slopes, sn_slopes, 
                     memory_slopes, hc_slopes,
                     em_obs, hc_obs, fc_obs, 
                     mem_time, hc_time, fc_time,
                     paste0("age.", 1:6),
                     paste0("memory.", 1:6),
                     paste0("adj_HIP_total.", 1:6),
                     paste0("HIP_total.", 1:6),
                     paste0("icv.", 1:6),
                     paste0("ec.", 1:6),
                     paste0("sn.", 1:6),
                     paste0("dmn.", 1:6)
                    )

# Drop people who are missing all necessary data

clean_data <- clean_data %>%
  filter(!(is.na(memory_slopes) & is.na(hc_slopes) & is.na(dmn_slopes))) %>% # Assumes that if they are missing one FC slope -> missing all
  filter(em_obs >= 2, hc_obs >= 2) # Assumes that they need to have at least two EM and two HC observations for analyses

# Drop people who have a FU time of less than 1.5 years

clean_data <- clean_data %>%
  filter(mem_time > 1.5)

```

Find the baseline and followup values because not everyone has tp1 data and follow-up could be tp2 - tp6

Note that this needs to be used carefully and any code comparing BL and FU values must make sure that the number of observations for the variables are the same because, for example, if someone has only two memory values but three HC values, this code will pull the tp1 and tp2 memory values but the tp1 and tp3 HC values so in this case the HC and memory FU could not be compared. 

```{r}
# Function to create baseline and followup columns dynamically
create_bl_fu_columns <- function(data) {
  # Variables/Patterns to process
  variables <- c("dmn", "ec", "sn", "adj_HIP_total", "memory", "age")
  
  # Create new columns
  for (var in variables) {
    data[[paste0(var, "_baseline")]] <- NA
    data[[paste0(var, "_followup")]] <- NA
  }

  # Helper function to assign baseline and followup values
  assign_columns <- function(row, var, original_data, data) {
    pattern <- paste0("^", var, "\\.\\d+$") # \\d selects any digit <10 at the end of str $
    indices <- grep(pattern, names(original_data))
    values <- original_data[row, indices, drop = FALSE]
    
    non_na_indices <- which(!is.na(values))

    if (length(non_na_indices) >= 1) {
      baseline_col <- paste0(var, "_baseline")
      baseline_index <- indices[non_na_indices[1]]
      data[[baseline_col]][row] <- original_data[row, baseline_index]
    }
    if (length(non_na_indices) > 1) {
      followup_col <- paste0(var, "_followup")
      followup_index <- indices[non_na_indices[length(non_na_indices)]]
      data[[followup_col]][row] <- original_data[row, followup_index]
    }
    
    return(data)
  }
  
  # Iterate over each row and variables
  for (i in 1:nrow(data)) {
    for (var in variables) {
      data <- assign_columns(i, var, data, data)
    }
  }
  
  return(data)
}

# Apply the function
clean_data <- clean_data %>%
  create_bl_fu_columns()

# Check that it is working correctly
dmn_check <- clean_data %>%
  select(id, matches("^dmn\\.\\d+$"), dmn_baseline, dmn_followup)

```

Look at the difference in demographics by cohort

```{r}
# MMSE
mmse_model <- aov(mmse ~ cohort, data = clean_data)
summary(mmse_model)

TukeyHSD(mmse_model)

# Age baseline
age_model <- aov(age_baseline ~ cohort, data = clean_data)
summary(age_model)

TukeyHSD(age_model)

# Education
edu_model <- aov(edu ~ cohort, data = clean_data)
summary(edu_model)

TukeyHSD(edu_model)

# FU time
# Create a new variable to represent average follow-up time 
clean_data <- clean_data%>%
  mutate(FU_time = rowMeans(select(., mem_time, hc_time, fc_time), na.rm = TRUE))

fu_time_model <- aov(FU_time ~ cohort, data = clean_data)
summary(fu_time_model)

# FU observations
# Create a new variable to represent average number of follow-ups 
clean_data <- clean_data%>%
  mutate(FU_obs = rowMeans(select(., em_obs, hc_obs, fc_obs), na.rm = TRUE))
fu_obs_model <- aov(FU_obs ~ cohort, data = clean_data)
summary(fu_obs_model)


```

Now write the data frames as CSVs
```{r}

write.csv(clean_data, file = "/tsd/p274/data/durable/projects/p027-cr_bm/Clean Data All Cohorts/Wide Format/Merged cohorts analysed data wide.csv")

```